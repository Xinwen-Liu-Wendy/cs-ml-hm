{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90437aa",
   "metadata": {},
   "source": [
    "### ML homework2  \n",
    "\n",
    "Name: Xinwen Liu       UNI: xl2796"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2cc97",
   "metadata": {},
   "source": [
    "1. (i) We want to prove that for any $x_i$ in the defined region $(-B, B)$ and for any binary labeling $y_i$, there is\n",
    "$$sign(\\vec{w}[1(B > \\gamma > x_i - \\alpha)]) = y_i$$\n",
    " which is equivilant to $$ \\vec{w} [1(B > \\gamma > x_i - \\alpha)] = y_i$$\n",
    " \n",
    " $$ y_i = \\begin{cases} \\vec{w} * 1 & B > \\gamma > x-\\alpha\\\\ \\vec{w} * 0 & otherwise\\end{cases} $$\n",
    "\n",
    "It can be seen that in the otherwise condition, $y_i$ has to be zero, which contradicts with the fact that yi can be any binary labeling.\n",
    "\n",
    "Thus, $B > \\gamma > x-\\alpha $ should always be true for any x in the defined (-B,B) region. Then we can simply put wi = yi to satisfy the above equation\n",
    "\n",
    "As $B>0, \\alpha > 0, -B < x < B, x-\\alpha < B$ is always true\n",
    "\n",
    "Another condition is that $\\gamma < B$\n",
    "\n",
    "To satisfy $ x - \\alpha < \\gamma, x < \\alpha + \\gamma \\ (\\gamma < B)$ \n",
    "\n",
    "If $\\alpha + \\gamma = B, x < \\alpha + \\gamma$ is always true\n",
    "\n",
    "Therfore, there always exist $\\alpha > 0$, which satisfies $\\alpha + \\gamma = B \\ (\\gamma < B) $, such that the mapping Φα,B can linearly separate any binary labeling of the n points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce0584",
   "metadata": {},
   "source": [
    "（ii) $$\\phi_\\alpha,_B (x) . \\phi_\\alpha,_B(x') = \\begin{cases} 1 & B > \\gamma > x-\\alpha,\\ B > \\gamma > x'-\\alpha \\\\ 0 & otherwise \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3d9bf",
   "metadata": {},
   "source": [
    "2. (i) Assume there are 2 dataspoints in the datasets: $x_1(1,0), y_1=1, x_2(-1,0), y_2=-1$ \n",
    "\n",
    "$ w_0 = (0,0)$, which does not make correct classification; \n",
    "\n",
    "$ w_1 = w_0 + y_1\\cdot x_1 = (1,0)$, now both x1 and x2 are correctly classfied.\n",
    "\n",
    "The number of mistake made is 1, which equals to $ (\\frac{R}{r} )^2 = (\\frac{\\sqrt{1}}{1})^2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06905a",
   "metadata": {},
   "source": [
    "(ii)$$ ||w_T||^2 = ||w_{T-1} + y_T{x_T}||^2 = ||{w_{T-1}}||^2 + 2y_T({w_{T-1}} . {x_T}) + ||y_T{x_T}||^2 \\le ||{w_{T-1}}||^2 + ||{x_T}||^2 ( y_T = {-1,1}) $$\n",
    "\n",
    "It can be inferred that $$ ||{w_T}||^2 \\le \\sum_{n=1}^{T}||{x_{it}}||^2 $$\n",
    "\n",
    "According to the given condition of the dataset $$ max_{xi\\in S}||(I-P)x_i|| \\le \\epsilon $$\n",
    "\n",
    "$$ max_{xi\\in S}||(I-P)x_i||^2 \\le \\epsilon^2 $$\n",
    "\n",
    "$$||(I-P)x_i||^2 = x_i^T(I^T-P^T)(I-P)x_i$$\\\n",
    "$$= x_i^T(I^TI - I^TP - P^TI  + P^TP)x_i$$\\\n",
    "$$= x_i^T(I-P^TP)x_i \\ (considering P^2 = P, P^T = P) $$\\\n",
    "$$ = ||x_i||^2 - ||Px_i||^2$$\n",
    "\n",
    "Thus, $$ max_{xi\\in S}||(I-P)x_i||^2 = max_{xi\\in S}(||x_i||^2 - ||Px_i||^2) \\le \\epsilon^2$$\n",
    "\n",
    "$$ ||x_i||^2 \\le \\epsilon^2 + ||Px_i||^2 $$\n",
    "\n",
    "$$ ||{w_T}||^2 \\le \\sum_{n=1}^{T}||{x_{it}}||^2 \\le \\epsilon^2 T + \\sum_{n=1}^{T}||Px_{it}||^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbf4de",
   "metadata": {},
   "source": [
    "(iii) $$ w_T \\cdot w^* = (w_{T-1} + y_Tx_T) \\cdot w^* = \\sum_{i=1}^{T}y_{it}x_{it} \\cdot w^* $$\n",
    " \n",
    " $$ (w_T \\cdot w^*)^2 = (\\sum_{i=1}^{T}y_{it}x_{it} \\cdot w^*)^2$$\n",
    " $$= \\sum_{i=1}^{T}(y_{it}x_{it} \\cdot w^*)^2 + \\sum_{i=1}^{T} \\sum_{j=i+1}^{T}(y_{it}x_{it} \\cdot w^*)(y_{jt}x_{jt} \\cdot w^*)$$ \n",
    " $$= \\sum_{i=1}^{T}(x_{it} \\cdot w^*)^2 + \\sum_{i=1}^{T} \\sum_{j=i+1}^{T}(y_{it}x_{it} \\cdot w^*)(y_{jt}x_{jt} \\cdot w^*) \\ (y_{it} \\in \\{-1,1\\})$$ \n",
    " \n",
    " Considering $$ ||Px_{it}||^2 = x_{it}^T P^T P x_{it} = x_{it}^T P^2 x_{it} = x_{it}^T P x_{it} = x_{it}^T w^* w^{*T} x_{it} = ||w^{*T}x_{it}||^2 =(x_{it} \\cdot w^*)^2 $$ \n",
    " \n",
    " and $$ y_{it}x_{it} \\cdot w^* \\ge \\gamma $$\n",
    " It can be inferred that $$ (w_T \\cdot w^*)^2 \\ge \\sum_{i=1}^{T}||Px_{it}||^2 + T(T-1) \\gamma^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ad790",
   "metadata": {},
   "source": [
    "(iv) $$ (w_T \\cdot w^*)^2 \\le ||w_T||^2 ||w^*||^2 $$\n",
    "\n",
    "Given (ii) an (iii), there is $$ \\sum_{i=1}^{T}||Px_{it}||^2 + T(T-1) \\gamma^2 \\le (w_T \\cdot w^*)^2 \\le ||w_T||^2 ||w^*||^2\\le \\epsilon^2 T + \\sum_{n=1}^{T}||Px_{it}||^2$$\n",
    "$$ T(T-1) \\gamma^2 \\le \\epsilon^2 T \\ (T \\ge 1)$$\n",
    "\n",
    "$$ (T-1) \\gamma^2 \\le \\epsilon^2 $$\n",
    "\n",
    "$$ T \\le (\\frac{\\epsilon}{\\gamma})^2 + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0ab3f",
   "metadata": {},
   "source": [
    "3. （i） For instance, for job application, if overall certain race ((e.g. Caucasian) is more likely to get the job compared with other races, even after removing the race feature from the classifier, it is possible to have other highly correlated features to the race in the dataset, which can make the classifier prefer Caucasian applicants. This will lead to unfairness, especially when people in other races are more capable of this job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209b0c0",
   "metadata": {},
   "source": [
    "(ii) For the left defintion, if $$ P_0[\\hat Y = 1] = P_1[\\hat Y = 1] $$\n",
    "$$ P(\\hat Y = 1|a = 0) = P(\\hat Y = 1|a = 1) $$\n",
    "\n",
    "For the right definition, $$ P(\\hat Y = 1) = P(\\hat Y = 1,a=0) + P(\\hat Y = 1,a=1)$$\n",
    "$$= P(\\hat Y = 1|a = 0)*P(a=0) + P(\\hat Y = 1|a = 1)*P(a=1)$$\n",
    "\n",
    "If given the condition $ P(\\hat Y = 1|a = 0) = P(\\hat Y = 1|a = 1), AND \\  P(a=0)+ P(a=1) =1 $,\n",
    "\n",
    "$$ P(\\hat Y = 1) = P(\\hat Y = 1|a = 0) * (P(a=0) + P(a=1)) $$\n",
    "$$= P(\\hat Y = 1|a = 0) $$ \n",
    "$$= P_0[\\hat Y = 1]$$\n",
    "$$= P_1[\\hat Y = 1]$$\n",
    "$$= P_a[\\hat Y = 1],  \\forall a\\in \\{0,1\\} $$ \n",
    "\n",
    "Therefore, the left and right definitions are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060cb53",
   "metadata": {},
   "source": [
    "(iii) For $ A \\in N, Y \\in R $, if \n",
    "$$ P_0[\\hat Y = y] = P_1[\\hat Y = y] = ... = P_N[\\hat Y = y], y \\in R $$\n",
    "\n",
    "$ P(\\hat Y = y) = P(\\hat Y = y,a=0) + P(\\hat Y = y,a=1) + ... +P(\\hat Y = y,a=N) $\n",
    "$= P(\\hat Y = y|a = 0)*P(a=0) + P(\\hat Y = y|a = 1)*P(a=1) + ... + P(\\hat Y = y|a = N)*P(a=N)$\n",
    "$= P(\\hat Y = y|a = 0)\\ (P(a=0) + P(a=1) + ... P(a=N))$\\\n",
    "$= P(\\hat Y = y|a = 0)$\\\n",
    "$= P_0[\\hat Y = y]$\\\n",
    "$= P_a[\\hat Y = y], \\forall a\\in N $\n",
    "\n",
    "Therefore, the generealized two equivilances are: \n",
    "$$ P_0[\\hat Y = y] = P_1[\\hat Y = y] = ... = P_N[\\hat Y = y], y \\in R \\iff P(\\hat Y = y)= P_a[\\hat Y = y], \\forall a\\in N, y \\in R $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab31575",
   "metadata": {},
   "source": [
    "(iv) see code\n",
    "(v) In terms of the accuracy, the \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
